{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb40af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Importing Neccessary dependencies\n",
    "import hub\n",
    "import sckit\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a80497",
   "metadata": {},
   "source": [
    "## The list of all popular datasets can be browsed through:\n",
    "\n",
    "activeloop list-datasets --workspace activeloop\n",
    "\n",
    "Supposing the entire datasets are appended into a form of list, hence the user gets to input their preffered dataset.\n",
    "\n",
    "For the case the dataset is not available in List of common datasets, The user gets to choose to upload their own.\n",
    "1. Using hub.load(): To load entire folder for datasets.\n",
    "2. Using hub.read(): To read a single file (eg. Numeric data)\n",
    "3. Defining an url: (From popular online sources of datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f52ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "extensions = ['json', 'png', 'jpg', 'jpeg', 'xml', 'yaml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de8707",
   "metadata": {},
   "outputs": [],
   "source": [
    " ##Specifying the Datasets to be the list of Datasets:\n",
    "preffered_dataset = input(\"Enter your dataset\") #Preffered_dataset defined the choice of dataset from the user\n",
    "id preffered_dataset in datasets:\n",
    "    wget.download(preffered_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765224b",
   "metadata": {},
   "source": [
    "# The dataset download usually takes place in form of ZipFile\n",
    "### After downloading, extracting is another necessary step\n",
    "\n",
    "The Zipfile module can extract the entire Zip contents downloaded using wget module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef9dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.Zipfile(downloaded file path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(r\"Current working directory\") #The current working directory can be easily found by checking properties\n",
    "                                                    #of the folder Absolute path of the directory is required for this case.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa2531",
   "metadata": {},
   "source": [
    "# Case 2: If the Preffered_dataset doesnt exist in the Datasets.\n",
    "\n",
    "## However, The user has reference to the Datasets by:\n",
    "\n",
    "### a. The user has the Dataset stored over local machine\n",
    "### b. The user has the Dataset link from any popular source over the internet with public access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4973effa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For the case being dataset is in local machine: \n",
    "import hub\n",
    "extensions = ['xml', 'yaml', 'txt', 'png', 'jpg', 'jpeg', 'json', 'csv', 'sqlite3'] #Extensions maybe of other kinds, however, \n",
    "                                                                                    #these are the most commonly used extensions\n",
    "if preffered_dataset.endswith(for i in extensions):\n",
    "    hub.read(preffered_dataset)\n",
    "else:\n",
    "    hub.load(preffered_dataset)\n",
    "#Please note preffered_dataset is the path/url provided by the user. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd890b31",
   "metadata": {},
   "source": [
    "# Note: For the case being url entered by User, the url can be validated by using Validator() itself, and above cell can be applied, using Wget to download the package.\n",
    "## For dataset being available in different formats (Csv filed/ Pictures) or Validated, Training Pipelines can also be defined accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63dfa0",
   "metadata": {},
   "source": [
    "#### The Pipeline method is defined to make your script interactive with your data and perform any necessary operations over it.\n",
    "For any normal Machine Learning model, Some of the most standard operations include Data Analysis, cleaning, standardization and scaling etc. \n",
    "The Pipelining can help initializing these operations at the time of interaction of data with script."
   ]
  },
  {
   "cell_type": "raw",
   "id": "58d6d41a",
   "metadata": {},
   "source": [
    "scale = ABC\n",
    "enc = PQR\n",
    "reg = XYZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621af48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case 1: Dataset being Csv:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#supposing that the ML algorithm being applied is XYZ regression, Scaling procedure is ABC, and PQR encoding\n",
    "hub.read(preffered_dataset)\n",
    "#cleaning and visualisation of dataset\n",
    "dt_cvt = make_coloum_transformer((enc,[\"Coloumns which need to be encoded\"]),(scale,['coloumns needed to e scaled']),remainder = passthrough)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06ad88",
   "metadata": {},
   "source": [
    "# The above representation defines the tuple of tuples, consisting of labels and values\n",
    "### The labels represent the technique (ex. type of encoding/ regression being used, whereas the values represent the target coloumns being processed by those techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a445025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(dt_cvt, XYZ)\n",
    "pipe.fit(X_train, Y_train)\n",
    "#The Dataset needs to be split into Test and Train values, to ensure that the training and validation can be done easily.\n",
    "#using common packages, the dataset can be split into percentages, eg. 70:30 or 80:20 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446283f7",
   "metadata": {},
   "source": [
    "#### For the same, Hyper-Parameter optimisation can be performed for best set of parameters and maximum achievable accuracy/ results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
